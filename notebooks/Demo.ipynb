{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo notebook for running the proposed method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import models\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from collections import Counter\n",
    "import warnings\n",
    "# Our files\n",
    "import train\n",
    "import utils\n",
    "import dataset\n",
    "\n",
    "plt.ion()\n",
    "plt.show()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "run = 0\n",
    "torch.manual_seed(run)\n",
    "torch.cuda.manual_seed_all(run)\n",
    "np.random.seed(run)\n",
    "random.seed(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cifar10\"  # other options \"cifar100\"\n",
    "data_path = '../datasets/'  # where the datasets have been saved\n",
    "noisy_rate = 0.4  # noise ratio, only applies to cifar; other values [0.2, 0.4, 0.6, 0.8]\n",
    "version = 0  # 3 versions of corrupted labels have been generated randomly [0, 1, 2]\n",
    "# Load corrupted labels\n",
    "corrupted_targets = np.load(\n",
    "    f\"../train_data/{dataset_name}_corrupted_{noisy_rate}_{version}.npy\")\n",
    "# for validation purposes, load ground truth\n",
    "true_targets = np.load(f\"../train_data/{dataset_name}_true.npy\")\n",
    "model_name = \"resnet18\"  # other options : \"moco\"\n",
    "loss_name = \"nfl_rce\"  # other options :\"elr\", \"ce\"\n",
    "arch = 'resnet18'  # model type for encoder\n",
    "use_validation = False\n",
    "use_protype = False\n",
    "classification_arch = 'linear'  # other options \"multilayer\"\n",
    "device = utils.get_device()  # device cpu/gpu\n",
    "# params is a dictionary placeholder for all input parameters\n",
    "params = utils.get_params(dataset_name,\n",
    "                          img_size=32,\n",
    "                          batch_size_classif=256,\n",
    "                          batch_size_representation=128,\n",
    "                          num_workers_classif=3,\n",
    "                          num_workers_representation=3,\n",
    "                          nb_classes=10,\n",
    "                          model_name=model_name,\n",
    "                          use_protype=use_protype,\n",
    "                          use_validation=use_validation,\n",
    "                          arch=arch,\n",
    "                          data_path=data_path,\n",
    "                          classification_arch=classification_arch)\n",
    "\n",
    "# unique name for the set of experiments\n",
    "exp = f\"demo_{dataset_name}_{noisy_rate}_{model_name}_{loss_name}_{version}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLOCK 1: contrastive pretraing and supervised learning\n",
    "\n",
    "### 1.1 Unsupervised pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input data\n",
    "label_data = {\n",
    "    \"labels\": corrupted_targets,\n",
    "    \"true_targets\": true_targets,\n",
    "    \"sup_sample_ids\": None,  # use no samples with supervised loss\n",
    "    \"unsup_sample_ids\": []  # exclude no samples from unsupervised loss\n",
    "}\n",
    "\n",
    "params['augmentation']['representation_train'] = 'moco'\n",
    "params['augmentation']['representation_train_strong'] = 'moco'\n",
    "params['adam'] = True\n",
    "name = f\"{exp}_representation_unsup\"\n",
    "\n",
    "model, criterion_sup = utils.get_model_and_criterion(dataset_name,\n",
    "                                                     loss_name=loss_name,\n",
    "                                                     model=model_name,\n",
    "                                                     p=params)\n",
    "\n",
    "result = train.representation_training(\n",
    "    model,\n",
    "    checkpoint_path_file=None,\n",
    "    label_data=label_data,\n",
    "    p=params,\n",
    "    epochs=2,\n",
    "    checkpoint_folder=f\"../data/models/{dataset_name}\",\n",
    "    name=name)\n",
    "with open(f'../data/results/{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "results = [result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input data\n",
    "label_data = {\"labels\": corrupted_targets, \"true_targets\": true_targets}\n",
    "name = f\"{exp}_supervised_full\"\n",
    "\n",
    "result = train.supervised_training(\n",
    "    model,\n",
    "    checkpoint_path_file=results[0][\"checkpoint_path_file\"],\n",
    "    loss_name=loss_name,\n",
    "    label_data=label_data,\n",
    "    p=params,\n",
    "    epochs=2,  # nb epochs to train entire model\n",
    "    checkpoint_folder=f\"../data/models/{dataset_name}\",\n",
    "    dataset_name=dataset_name,\n",
    "    name=name,\n",
    "    nb_epochs_output_training=2,  # nb epochs to train only classification head\n",
    "    finetune_lr=False)\n",
    "with open(f'../data/results/{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLOCK 2: Method improvements\n",
    "### 2.1 GMM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [result]\n",
    "# placeholder for input data\n",
    "label_data = {\"labels\": corrupted_targets, \"true_targets\": true_targets}\n",
    "# pass pseudo labels in fields \"pred_train\" and \"pred_val\"\n",
    "pretrain_result = results[0]\n",
    "if \"pred_train\" in pretrain_result[\"model_output\"]:\n",
    "    label_data[\"pred_train\"] = pretrain_result[\"model_output\"][\"pred_train\"]\n",
    "if \"pred_val\" in pretrain_result[\"model_output\"]:\n",
    "    label_data[\"pred_val\"] = pretrain_result[\"model_output\"][\"pred_val\"]\n",
    "\n",
    "name = f\"{exp}_gmm\"\n",
    "\n",
    "result = train.supervised_training(\n",
    "    model,\n",
    "    checkpoint_path_file=results[-1][\"checkpoint_path_file\"],\n",
    "    loss_name=loss_name,\n",
    "    label_data=label_data,\n",
    "    p=params,\n",
    "    epochs=0,  # nb epochs to train entire model\n",
    "    checkpoint_folder=f\"../data/models/{dataset_name}\",\n",
    "    dataset_name=dataset_name,\n",
    "    name=name,\n",
    "    nb_epochs_output_training=2,  # nb epochs to train only classification head\n",
    "    finetune_lr=True)\n",
    "with open(f'../data/results/{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Supervised representation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input data\n",
    "label_data = {\n",
    "    \"labels\": corrupted_targets,\n",
    "    \"true_targets\": true_targets,\n",
    "    \"sup_sample_ids\": None, \n",
    "    \"unsup_sample_ids\": []  \n",
    "}\n",
    "# pass pseudo labels in fields \"pred_train\" and \"pred_val\"\n",
    "pretrain_result = results[0]\n",
    "if \"pred_train\" in pretrain_result[\"model_output\"]:\n",
    "    label_data[\"pred_train\"] = pretrain_result[\"model_output\"][\"pred_train\"]\n",
    "if \"pred_val\" in pretrain_result[\"model_output\"]:\n",
    "    label_data[\"pred_val\"] = pretrain_result[\"model_output\"][\"pred_val\"]\n",
    "\n",
    "label_data[\"weights\"] = torch.FloatTensor(results[-1][\"weights\"]).to(device)\n",
    "#clip data to avoid nan loss\n",
    "label_data[\"weights\"][torch.where(label_data[\"weights\"] < 0.01)[0]] = 0.01\n",
    "params['augmentation']['representation_train'] = 'moco'\n",
    "params['augmentation']['representation_train_strong'] = 'moco'\n",
    "params['adam'] = True\n",
    "name = f\"{exp}_representation_sup\"\n",
    "\n",
    "model, criterion_sup = utils.get_model_and_criterion(dataset_name,\n",
    "                                                     loss_name=loss_name,\n",
    "                                                     model=model_name,\n",
    "                                                     p=params)\n",
    "\n",
    "result = train.representation_training(\n",
    "    model,\n",
    "    checkpoint_path_file=results[0][\"checkpoint_path_file\"],\n",
    "    label_data=label_data,\n",
    "    p=params,\n",
    "    epochs=2,  # nb epochs to train entire model\n",
    "    checkpoint_folder=f\"../data/models/{dataset_name}\",\n",
    "    name=name)\n",
    "with open(f'../data/results/{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input data\n",
    "label_data = {\"labels\": corrupted_targets, \"true_targets\": true_targets}\n",
    "# pass pseudo labels in fields \"pred_train\" and \"pred_val\"\n",
    "pretrain_result = results[0]\n",
    "if \"pred_train\" in pretrain_result[\"model_output\"]:\n",
    "    label_data[\"pred_train\"] = pretrain_result[\"model_output\"][\"pred_train\"]\n",
    "if \"pred_val\" in pretrain_result[\"model_output\"]:\n",
    "    label_data[\"pred_val\"] = pretrain_result[\"model_output\"][\"pred_val\"]\n",
    "params['adam'] = False\n",
    "name = f\"{exp}_sup_final\"\n",
    "\n",
    "model, criterion_sup = utils.get_model_and_criterion(dataset_name,\n",
    "                                                     loss_name=loss_name,\n",
    "                                                     model=model_name,\n",
    "                                                     p=params)\n",
    "result = train.supervised_training(\n",
    "    model,\n",
    "    checkpoint_path_file=results[-1][\"checkpoint_path_file\"],\n",
    "    loss_name=loss_name,\n",
    "    label_data=label_data,\n",
    "    p=params,\n",
    "    epochs=2,  # nb epochs to train entire model\n",
    "    checkpoint_folder=f\"../data/models/{dataset_name}\",\n",
    "    dataset_name=dataset_name,\n",
    "    name=name,\n",
    "    nb_epochs_output_training=2,  # nb epochs to train only classification head\n",
    "    finetune_lr=False)\n",
    "with open(f'../data/results/{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline experiment to get the score of the orginal loss without contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{exp}_baseline_corrupted\"\n",
    "# placeholder for input data\n",
    "label_data = {\"labels\": corrupted_targets, \"true_targets\": true_targets}\n",
    "# Instantiate model and supervised loss\n",
    "model, criterion_sup = utils.get_model_and_criterion(dataset_name,\n",
    "                                                     loss_name=loss_name,\n",
    "                                                     model=model_name,\n",
    "                                                     p=params)\n",
    "\n",
    "result = train.supervised_training(\n",
    "    model,\n",
    "    checkpoint_path_file=None,\n",
    "    loss_name=loss_name,\n",
    "    label_data=label_data,\n",
    "    p=params,\n",
    "    epochs=2,  # nb epochs to train entire model\n",
    "    checkpoint_folder=f\"../data/models/{dataset_name}\",\n",
    "    dataset_name=dataset_name,\n",
    "    name=name,\n",
    "    nb_epochs_output_training=0,  # nb epochs to train only classification head\n",
    "    finetune_lr=False)\n",
    "with open(f'../data/results/{name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
